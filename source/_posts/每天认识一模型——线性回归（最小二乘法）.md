---
title: 每天认识一模型——线性回归（最小二乘法）
date: 2025-07-13 11:19:26
categories: 数模备战
cover: /img/post3.jpg
---
为了备战9月的数模竞赛，从今天开始每天学习一个数学模型，看看自己能坚持多久。今天学习的是最基础的模型线性回归，也叫最小二乘法。

# 一、线性回归引入
对于一元线性回归，我们学高中数学的时候都学过，其实是把一个自变量x和一个因变量y用线性关系联系起来，我们可以通过最小二乘法去算出参数k和b，从而得到y与x的关系。但在现实中，我们在处理问题的时候数据一般会比较复杂，影响因变量的因素往往不止一个（比如房价不仅和面积相关，还和地段、楼层、房龄等有关）。而线性回归的意思是我们把这些变量引入一个线性模型，假设它们的关系对参数是线性的，从而去算出参数k1,k2......kn，继而得到数学关系。

# 二、线性回归的优缺点
## 线性回归是什么
线性回归是一种**监督学习模型**，用于建模自变量（特征）与因变量（目标变量）之间的线性关系。其核心假设是：因变量可以通过自变量的线性组合加上误差项来表示。
这里提一嘴监督学习模型，它是训练数据带有标签，如房价、面积、地段、楼层这些。而无监督学习则是数据没有标签，你自己都不知道它们是啥，又电脑自己发现数据本身的规律（如聚类、降维）
这里还有个误区，可能认为线性回归的自变量只能是一次方，实际上即使包含自变量的平方项（如x²）、交叉项（如x1​x2）等，模型依然会为这些项求解对应的参数。
## 线性回归的优点
**1、模型简单，解释性强**
线性回归的数学形式直观（如 y=k1x1+k2x2+...+b），参数（系数ki）具有明确的实际意义，表示随x增大而增大或减少
**2、计算高效，易于实现**
线性回归的参数求解有成熟的数学公式（如最小二乘法的解析解），无需复杂的迭代优化，对大规模数据也能快速处理。
## 线性回归的缺点
**1、对数据线性关系的假设严格**
线性回归的基础是假设变量之间是线性关系，但实际关系是非线性的（如指数关系、周期性关系），直接使用线性模型会导致拟合效果差（欠拟合）。例如，用线性模型预测 “人口增长”（实际可能是指数增长）或 “气温随季节变化”（实际是周期性波动），误差会很大。
**2、对异常值敏感**
线性回归通过最小化 “误差平方和” 求解参数，而平方项会放大异常值（离群点）的影响，导致模型被少数异常值 “带偏”。例如，在薪资预测中，若数据包含一个错误的 “年薪 1 亿元” 样本，线性模型可能会显著高估高学历人群的薪资
**3、难以处理多重共线性**
当自变量之间存在高度相关性（如 “身高” 与 “体重”、“广告费用” 与 “营销投入”）时，会导致参数估计不稳定（系数波动大、符号异常），甚至无法准确解释自变量的影响。我的理解是本来自变量之间就有高度相关性，比如一般身高增大，体重也会增大，那么当身高增大的时候，模型就不知道是身高增大影响因变量，还是身高增大导致体重增大再影响自变量。

# 三、线性回归的实现
其实线性回归我们可以用线性代数的知识自己手搓，但计算矩阵什么的十分麻烦，在这里就直接把python代码放出来，由deepseek提供：
``` bash
# 导入必要的库
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# 1. 数据准备
# 替换为你的数据集路径
data = pd.read_csv('your_dataset.csv')  # 支持csv、excel等格式

# 查看数据前5行，了解数据结构
print("数据集预览：")
print(data.head())

# 2. 特征选择
# 假设最后一列是目标变量，其余是特征（根据你的数据调整）
X = data.iloc[:, :-1].values  # 特征矩阵（除去最后一列的所有列）
y = data.iloc[:, -1].values   # 目标变量（最后一列）

# 如果你的特征和目标列位置不同，请手动指定：
# X = data[['feature1', 'feature2', ...]]  # 替换为实际特征列名
# y = data['target_column']               # 替换为目标列名

# 3. 数据拆分（训练集和测试集）
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,   # 测试集比例（通常0.2-0.3）
    random_state=42   # 随机种子，确保结果可复现
)

# 4. 创建并训练模型
model = LinearRegression()  # 创建线性回归模型
model.fit(X_train, y_train)  # 在训练集上训练模型

# 5. 模型评估
y_pred = model.predict(X_test)  # 在测试集上进行预测

# 计算性能指标
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\n模型性能评估：")
print(f"均方误差(MSE): {mse:.4f}")
print(f"均方根误差(RMSE): {rmse:.4f}")
print(f"决定系数(R²): {r2:.4f}")

# 6. 查看模型参数
print("\n模型参数：")
print(f"截距(Intercept): {model.intercept_:.4f}")
print("回归系数(Coefficients):")
for i, coef in enumerate(model.coef_):
    print(f"  特征{i+1}: {coef:.4f}")

# 7. 使用模型进行新预测（示例）
# 假设有新数据点 [feature1, feature2, ...]
new_data = np.array([[5.1, 3.5, 1.4, 0.2]])  # 替换为你的特征值
prediction = model.predict(new_data)
print(f"\n新数据预测结果: {prediction[0]:.4f}")

# 8. 保存模型（可选）
import joblib
joblib.dump(model, 'linear_regression_model.pkl')  # 保存模型
# 加载模型：loaded_model = joblib.load('linear_regression_model.pkl')

#9. 数据可视化（可根据自己需求添加）
```
# 四、注意事项
### 1、定类数据的处理：
在线性回归模型中，如果输入变量中包含定类数据，这些数据需要被适当地处理以用于回归分析。如果输入变量是定类数据，则要进行进行哑变量化处理，以确保数据可以被正确地用于线性回归模型。​每个哑变量对应一个类别，用 “1” 表示属于该类别，“0” 表示不属于。例如是否精装房这个因变量再进行线性回归时难以处理，我们可以用1代表是，0代表不是。
### 2、计算方法与差异：
以上代码使用最小二乘法来计算线性回归的参数。但需要注意的是，还有一种方法叫做梯度下降法，它依赖于初始值的选择和迭代过程，适用于大规模数据，需手动调参（如学习率、迭代次数）。
### 3、模型性能评估：
这里有几个参数可以反应模型的性能：
**（1）均方误差（MSE，Mean Squared Error）**：衡量的是模型预测值与真实值之间的平均误差大小。值越小说明预测值与真实值的偏差越小，模型拟合效果越好。值越大说明预测误差越大，模型可靠性越低。
**（2）均方根误差（RMSE，Root Mean Squared Error）**：RMSE 是 MSE 的平方根，它解决了 MSE 单位不直观的问题，其单位与目标变量一致。值越小说明预测值与真实值的平均偏差越小，模型预测精度越高。
**（3）决定系数（R²，R-squared）**：R² 衡量模型对目标变量变异的 “解释能力”，R²越接近1越好。
R² 越接近 1：说明模型能解释的变异越多（即预测值与真实值越接近），拟合效果越好。
R² 接近 0：说明模型几乎没有解释能力（预测效果和 “直接用均值猜测” 差不多）。
R² 为负：通常是模型效果极差（甚至不如 “用均值猜测”），可能是数据异常或模型选择错误导致。
**（4）回归系数的 p 值（p-value）**：衡量 “回归系数是否显著不为 0” 的概率（通过 t 检验计算）。有些变量可能关系不显著，是干扰项。
p 值 <0.05（通常阈值）：认为该特征与目标变量的线性关系 “统计显著”（不太可能是随机因素导致）。
p 值 ≥ 0.05：特征与目标的关系可能不显著，需考虑是否保留该特征。
**（5）VIF（方差膨胀因子，Variance Inflation Factor）**：衡量特征间多重共线性的指标，反映某个特征被其他特征 “解释” 的程度。在上文的缺点处提到过。
VIF<5：共线性很弱（严格标准）。
5≤VIF<10：共线性较弱，可接受。
VIF≥10：共线性严重，需处理（如删除特征、正则化）。
### 还有一些检验模型性能的参数，例如F检验、B 和标准误等等，作者也还没弄清楚，但是Ai帮我总结出以下流程
#### 模型整体是否有用？ → F 检验 P<0.05 ✔️
#### 特征间是否共线性严重？ → 所有 VIF<10 ✔️
#### 哪些特征真正有影响？ → 筛选 P<0.05 的特征（房间平方、房龄），剔除无意义的（楼层、配套电梯需再验证） ✔️
#### 模型拟合效果如何？ → R²=0.913（解释 91.3% 变异），拟合优 ✔️
#### 预测误差是否可接受？ → 需补充 MSE/RMSE 验证（假设测试集误差小） ✔️
#### 模型假设是否满足？ → 残差无明显规律（补充验证） ✔️

# 五、总结
线性回归一般适用于**预测数值型结果**（预测房价、股票价格、化学反应的产率、设备的运行寿命等）或**建立变量间的关系模型**（研究教育程度与收入水平之间的关系、人口增长与经济发展之间的关系、广告投入与产品销量之间的关系等）
数据特征为变量间存在线性关系、数据中噪声较小、数据具有独立性、特征之间多重共线性程度低。在我们运用该模型之前，可以把数据喂给Ai，让Ai判断该问题是否适合线性回归模型。